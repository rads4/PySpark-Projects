{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7519d2-2e67-4f1d-9391-0822eaab00c4",
   "metadata": {},
   "source": [
    "### Handling Missing Values:\n",
    "- Dropping Column\n",
    "- Dropping Rows\n",
    "- Various parameters in dropping functionalities\n",
    "- Handling missing values by mean, median & mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f655b2-eead-42ca-9ac3-66c60fcd6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Handling missing values').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc7ef2e-46ce-41de-a848-57080a2974fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset\n",
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18873360-de90-4322-a90d-a7d17cb3e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Radhika|  23|         8| 30000|\n",
      "| Shivam|  24|        10| 50000|\n",
      "|  Aashi|  18|         2| 15000|\n",
      "| Mayank|  19|         4| 15000|\n",
      "| Millie|  26|         6| 40000|\n",
      "|    Tom|  30|        13| 65000|\n",
      "| Shreya|NULL|      NULL| 42000|\n",
      "|   NULL|  34|        15| 38000|\n",
      "|   NULL|  36|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f335d8a-0d44-4305-8b0a-c1fe8dfabe78",
   "metadata": {},
   "source": [
    "### Dropping Columns/Rows of missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b34b6c3-d5e6-4b44-a1c4-c54528601d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  23|         8| 30000|\n",
      "|  24|        10| 50000|\n",
      "|  18|         2| 15000|\n",
      "|  19|         4| 15000|\n",
      "|  26|         6| 40000|\n",
      "|  30|        13| 65000|\n",
      "|NULL|      NULL| 42000|\n",
      "|  34|        15| 38000|\n",
      "|  36|      NULL|  NULL|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping a column\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7eaff7d-6811-41d0-8e1c-7ce106a2b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Radhika|  23|         8| 30000|\n",
      "| Shivam|  24|        10| 50000|\n",
      "|  Aashi|  18|         2| 15000|\n",
      "| Mayank|  19|         4| 15000|\n",
      "| Millie|  26|         6| 40000|\n",
      "|    Tom|  30|        13| 65000|\n",
      "| Shreya|NULL|      NULL| 42000|\n",
      "|   NULL|  34|        15| 38000|\n",
      "|   NULL|  36|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa9c4d4-8642-4a4b-834c-48e73768d4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Radhika| 23|         8| 30000|\n",
      "| Shivam| 24|        10| 50000|\n",
      "|  Aashi| 18|         2| 15000|\n",
      "| Mayank| 19|         4| 15000|\n",
      "| Millie| 26|         6| 40000|\n",
      "|    Tom| 30|        13| 65000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping specific rows\n",
    "df_pyspark.na.drop().show()\n",
    "\n",
    "#using na with drop, will remove all the NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524c34a-87fa-4199-9867-77ffb18461d4",
   "metadata": {},
   "source": [
    "### How:\n",
    "- If (how=any)\n",
    "-> dropped if it contains any null, (by default)\n",
    "\n",
    "- If (how=all)\n",
    "-> dropped if all values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ebd4c9-0382-441c-9493-14c6a9802f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Radhika| 23|         8| 30000|\n",
      "| Shivam| 24|        10| 50000|\n",
      "|  Aashi| 18|         2| 15000|\n",
      "| Mayank| 19|         4| 15000|\n",
      "| Millie| 26|         6| 40000|\n",
      "|    Tom| 30|        13| 65000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how=any\n",
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d208c81c-7d83-4729-80ee-245273d199d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Radhika|  23|         8| 30000|\n",
      "| Shivam|  24|        10| 50000|\n",
      "|  Aashi|  18|         2| 15000|\n",
      "| Mayank|  19|         4| 15000|\n",
      "| Millie|  26|         6| 40000|\n",
      "|    Tom|  30|        13| 65000|\n",
      "| Shreya|NULL|      NULL| 42000|\n",
      "|   NULL|  34|        15| 38000|\n",
      "|   NULL|  36|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how=all\n",
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99b4b8-8946-4739-b38a-187a3a104e8f",
   "metadata": {},
   "source": [
    "### Threshold:\n",
    "Sets the limit for non-null values\n",
    "\n",
    "- If thresh=2, then atleast 2 non-null values must be present in the row, if not then dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6007db-d839-4acf-aa33-ac9d599d54ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Radhika|  23|         8| 30000|\n",
      "| Shivam|  24|        10| 50000|\n",
      "|  Aashi|  18|         2| 15000|\n",
      "| Mayank|  19|         4| 15000|\n",
      "| Millie|  26|         6| 40000|\n",
      "|    Tom|  30|        13| 65000|\n",
      "| Shreya|NULL|      NULL| 42000|\n",
      "|   NULL|  34|        15| 38000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()\n",
    "#drops the last row because it just contains 1 Non-null value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dbe8ae-979f-40bd-b981-92d17512bb43",
   "metadata": {},
   "source": [
    "### Subset:\n",
    "Dropping values from a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a18079d-590b-4c15-aed5-b3b501712b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Radhika| 23|         8| 30000|\n",
      "| Shivam| 24|        10| 50000|\n",
      "|  Aashi| 18|         2| 15000|\n",
      "| Mayank| 19|         4| 15000|\n",
      "| Millie| 26|         6| 40000|\n",
      "|    Tom| 30|        13| 65000|\n",
      "|   NULL| 34|        15| 38000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\", subset=['Experience']).show()  \n",
    "#drops null values from the Experience column only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7f21b-b45f-4c65-b105-5a7be85aa956",
   "metadata": {},
   "source": [
    "### Filling the Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc058e0-4785-425d-a03f-0736198ac5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| Age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|       Radhika|  23|         8| 30000|\n",
      "|        Shivam|  24|        10| 50000|\n",
      "|         Aashi|  18|         2| 15000|\n",
      "|        Mayank|  19|         4| 15000|\n",
      "|        Millie|  26|         6| 40000|\n",
      "|           Tom|  30|        13| 65000|\n",
      "|        Shreya|NULL|      NULL| 42000|\n",
      "|Missing Values|  34|        15| 38000|\n",
      "|Missing Values|  36|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using fill - (value,subset)\n",
    "#using value only\n",
    "\n",
    "#for string columns\n",
    "df_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344c3235-80ab-46f9-9a8d-040235fb1a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Radhika| 23|         8| 30000|\n",
      "| Shivam| 24|        10| 50000|\n",
      "|  Aashi| 18|         2| 15000|\n",
      "| Mayank| 19|         4| 15000|\n",
      "| Millie| 26|         6| 40000|\n",
      "|    Tom| 30|        13| 65000|\n",
      "| Shreya|  0|         0| 42000|\n",
      "|   NULL| 34|        15| 38000|\n",
      "|   NULL| 36|         0|     0|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for numerical columns\n",
    "df_pyspark.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60f03352-a2a7-4930-946c-4564442a4916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----------+------+\n",
      "|          Name|Age|Experience|Salary|\n",
      "+--------------+---+----------+------+\n",
      "|       Radhika| 23|         8| 30000|\n",
      "|        Shivam| 24|        10| 50000|\n",
      "|         Aashi| 18|         2| 15000|\n",
      "|        Mayank| 19|         4| 15000|\n",
      "|        Millie| 26|         6| 40000|\n",
      "|           Tom| 30|        13| 65000|\n",
      "|        Shreya|  0|         0| 42000|\n",
      "|Missing Values| 34|        15| 38000|\n",
      "|Missing Values| 36|         0|     0|\n",
      "+--------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for both String and Numerical columns/combined filling\n",
    "\n",
    "df_filling = df_pyspark.na.fill('Missing Values')\n",
    "df_filling.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8bf5db6-abbe-483d-a888-ff9186bfa0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Radhika|  23|         8| 30000|\n",
      "| Shivam|  24|        10| 50000|\n",
      "|  Aashi|  18|         2| 15000|\n",
      "| Mayank|  19|         4| 15000|\n",
      "| Millie|  26|         6| 40000|\n",
      "|    Tom|  30|        13| 65000|\n",
      "| Shreya|NULL|         0| 42000|\n",
      "|   NULL|  34|        15| 38000|\n",
      "|   NULL|  36|         0|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using fill in a specific column\n",
    "#using value + subset\n",
    "df_pyspark.na.fill(0, 'Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ce95cf-1e53-4231-a59e-0eb95c453107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|Radhika| 23|         8| 30000|\n",
      "| Shivam| 24|        10| 50000|\n",
      "|  Aashi| 18|         2| 15000|\n",
      "| Mayank| 19|         4| 15000|\n",
      "| Millie| 26|         6| 40000|\n",
      "|    Tom| 30|        13| 65000|\n",
      "| Shreya|  0|         0| 42000|\n",
      "|   NULL| 34|        15| 38000|\n",
      "|   NULL| 36|         0|  NULL|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(0, ['Experience', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "527b6fc2-65fc-4ac0-8d75-12eb59d16477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Radhika|  23|         8| 30000|\n",
      "| Shivam|  24|        10| 50000|\n",
      "|  Aashi|  18|         2| 15000|\n",
      "| Mayank|  19|         4| 15000|\n",
      "| Millie|  26|         6| 40000|\n",
      "|    Tom|  30|        13| 65000|\n",
      "| Shreya|NULL|      NULL| 42000|\n",
      "|   NULL|  34|        15| 38000|\n",
      "|   NULL|  36|      NULL|  NULL|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c88e9-dd9e-4153-b65a-806a8cc5a6a6",
   "metadata": {},
   "source": [
    "### Filling Missing values using Mean, Median & Mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71617e6a-de39-4082-874a-a86c687e0d25",
   "metadata": {},
   "source": [
    "Mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeeefa8b-82de-4595-977f-49e18c6ca4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Imputer function \n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age', 'Experience', 'Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Experience', 'Salary']]).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2a409e8-a442-4d77-97ed-4d68068700bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|Radhika|  23|         8| 30000|         23|                 8|         30000|\n",
      "| Shivam|  24|        10| 50000|         24|                10|         50000|\n",
      "|  Aashi|  18|         2| 15000|         18|                 2|         15000|\n",
      "| Mayank|  19|         4| 15000|         19|                 4|         15000|\n",
      "| Millie|  26|         6| 40000|         26|                 6|         40000|\n",
      "|    Tom|  30|        13| 65000|         30|                13|         65000|\n",
      "| Shreya|NULL|      NULL| 42000|         26|                 8|         42000|\n",
      "|   NULL|  34|        15| 38000|         34|                15|         38000|\n",
      "|   NULL|  36|      NULL|  NULL|         36|                 8|         36875|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding Imputation Columns to df\n",
    "#fit & transform\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3c508-dea4-446a-845a-b52f6c6ea9bf",
   "metadata": {},
   "source": [
    "Median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d3c18a7-f7cb-4d2c-a754-39f33b34886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age', 'Experience', 'Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Experience', 'Salary']]).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55094b54-30a0-473f-b57b-72c66bfc2e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|Radhika|  23|         8| 30000|         23|                 8|         30000|\n",
      "| Shivam|  24|        10| 50000|         24|                10|         50000|\n",
      "|  Aashi|  18|         2| 15000|         18|                 2|         15000|\n",
      "| Mayank|  19|         4| 15000|         19|                 4|         15000|\n",
      "| Millie|  26|         6| 40000|         26|                 6|         40000|\n",
      "|    Tom|  30|        13| 65000|         30|                13|         65000|\n",
      "| Shreya|NULL|      NULL| 42000|         24|                 8|         42000|\n",
      "|   NULL|  34|        15| 38000|         34|                15|         38000|\n",
      "|   NULL|  36|      NULL|  NULL|         36|                 8|         38000|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceee271-3408-4a7f-ad37-9bd71edff988",
   "metadata": {},
   "source": [
    "Mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3241b000-9481-41ea-97fa-87428aeeeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age', 'Experience', 'Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Experience', 'Salary']]).setStrategy(\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e190d2b-6243-4d23-9d74-a7e55b99c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|Radhika|  23|         8| 30000|         23|                 8|         30000|\n",
      "| Shivam|  24|        10| 50000|         24|                10|         50000|\n",
      "|  Aashi|  18|         2| 15000|         18|                 2|         15000|\n",
      "| Mayank|  19|         4| 15000|         19|                 4|         15000|\n",
      "| Millie|  26|         6| 40000|         26|                 6|         40000|\n",
      "|    Tom|  30|        13| 65000|         30|                13|         65000|\n",
      "| Shreya|NULL|      NULL| 42000|         18|                 2|         42000|\n",
      "|   NULL|  34|        15| 38000|         34|                15|         38000|\n",
      "|   NULL|  36|      NULL|  NULL|         36|                 2|         15000|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
